{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Machine Learning Models\n",
    "\n",
    "In this notebook we will be using general machine learning models to predict the sentiment of a tweet. These models will be using data converted by either CountVectorizer or TFIDFVectorizer objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('clean_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_tweets'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_tweets</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id respond go</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sooo sad miss san diego</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boss bulli</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>interview leav alon</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>son whi couldnt put releas whatev alreadi bought</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24457</th>\n",
       "      <td>wish whatev could come see denver husband lost...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24458</th>\n",
       "      <td>ive wonder rake client ha made clear net onli ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24459</th>\n",
       "      <td>yay good enjoy break probabl need hectic weeke...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24460</th>\n",
       "      <td>wa worth</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24461</th>\n",
       "      <td>thi flirt go atg smile yay hug</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24424 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            clean_tweets sentiment\n",
       "0                                         id respond go    neutral\n",
       "1                               sooo sad miss san diego   negative\n",
       "2                                            boss bulli   negative\n",
       "3                                   interview leav alon   negative\n",
       "4      son whi couldnt put releas whatev alreadi bought   negative\n",
       "...                                                  ...       ...\n",
       "24457  wish whatev could come see denver husband lost...  negative\n",
       "24458  ive wonder rake client ha made clear net onli ...  negative\n",
       "24459  yay good enjoy break probabl need hectic weeke...  positive\n",
       "24460                                          wa worth   positive\n",
       "24461                    thi flirt go atg smile yay hug    neutral\n",
       "\n",
       "[24424 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['clean_tweets','sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "## Creating X and y variables with CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3), min_df=5)\n",
    "X = vectorizer.fit_transform(df['clean_tweets'])\n",
    "X = X.tocsc() \n",
    "y = df['sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3498643738164696\n",
      "0.3572159672466735\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "#split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.20, random_state=1)\n",
    "\n",
    "dclf = DummyClassifier(strategy='most_frequent')\n",
    "dclf.fit(X_train,y_train)\n",
    "y_pred = dclf.predict(X_test)\n",
    "print(dclf.score(X_train,y_train))\n",
    "print(dclf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00      1510\n",
      "     neutral       0.00      0.00      0.00      1630\n",
      "    positive       0.36      1.00      0.53      1745\n",
      "\n",
      "    accuracy                           0.36      4885\n",
      "   macro avg       0.12      0.33      0.18      4885\n",
      "weighted avg       0.13      0.36      0.19      4885\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopher/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer Models\n",
    "\n",
    "CountVectorizer uses a bag of words model, which takes a count of all the words in a document. These words are then tokenized and encoded as vectors of term/token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "## Creating X and y variables with CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3), min_df=5)\n",
    "X = vectorizer.fit_transform(df['clean_tweets'])\n",
    "X = X.tocsc() \n",
    "y = df['sentiment']\n",
    "\n",
    "\n",
    "#split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.20, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.70\n",
      "Test Accuracy: 0.70\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'gamma': 'scale', 'kernel': 'sigmoid'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svc = SVC()\n",
    "param_grid = {'C': [ 1, 10, 20],\n",
    "             'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "              'gamma' : ['scale', 'auto']}\n",
    "\n",
    "\n",
    "gs = GridSearchCV(estimator=svc, param_grid=param_grid, cv=3, scoring = 'accuracy')\n",
    "gs.fit(X_train, y_train)\n",
    "y_pred = gs.predict(X_test)\n",
    "print(\"Training Accuracy: {:.2f}\".format(gs.score(X_train, y_train)))\n",
    "print(\"Test Accuracy: {:.2f}\".format(gs.score(X_test, y_test)))\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6982681042860944\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.62      0.68      1510\n",
      "     neutral       0.60      0.72      0.65      1630\n",
      "    positive       0.77      0.74      0.76      1745\n",
      "\n",
      "    accuracy                           0.70      4885\n",
      "   macro avg       0.71      0.70      0.70      4885\n",
      "weighted avg       0.71      0.70      0.70      4885\n",
      "\n",
      "[[ 942  428  140]\n",
      " [ 219 1175  236]\n",
      " [  87  363 1295]]\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_test,y_pred, average='macro'))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Multinomial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7598648856133886 Test Accuracy: 0.6165813715455476 alpha: 0.1\n",
      "Training Accuracy: 0.7469676032550284 Test Accuracy: 0.6327533265097236 alpha: 1\n",
      "Training Accuracy: 0.7293106095501305 Test Accuracy: 0.6452405322415558 alpha: 5\n",
      "Training Accuracy: 0.7165156865755669 Test Accuracy: 0.6499488229273286 alpha: 10\n",
      "Training Accuracy: 0.6707098623266288 Test Accuracy: 0.6249744114636643 alpha: 50\n"
     ]
    }
   ],
   "source": [
    "## finding best value for alpha\n",
    "alphas = [.1, 1, 5, 10, 50]\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "for alpha in alphas:\n",
    "    nbc = MultinomialNB(alpha=alpha)\n",
    "    nbc.fit(X_train, y_train)\n",
    "    y_pred = nbc.predict(X_test)\n",
    "    print(\"Training Accuracy:\" ,nbc.score(X_train, y_train), \"Test Accuracy:\",nbc.score(X_test, y_test), 'alpha:',alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.72\n",
      "Test Accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "## naive bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#initiate classifier\n",
    "nbc = MultinomialNB(alpha=10)\n",
    "nbc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nbc.predict(X_test)\n",
    "\n",
    "print(\"Training Accuracy: {:.2f}\".format(nbc.score(X_train, y_train)))\n",
    "print(\"Test Accuracy: {:.2f}\".format(nbc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6427851818665348\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.65      0.70      0.67      1510\n",
      "     neutral       0.59      0.47      0.53      1630\n",
      "    positive       0.69      0.77      0.73      1745\n",
      "\n",
      "    accuracy                           0.65      4885\n",
      "   macro avg       0.64      0.65      0.64      4885\n",
      "weighted avg       0.64      0.65      0.64      4885\n",
      "\n",
      "[[1064  276  170]\n",
      " [ 433  772  425]\n",
      " [ 146  260 1339]]\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_test,y_pred, average='macro'))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier with Grid Seach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.76\n",
      "Test Accuracy: 0.68\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 30,\n",
       " 'max_features': 'sqrt',\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "param_grid = {'max_depth': [10,20,30],\n",
    "             'max_features': ['sqrt', 'log2'],\n",
    "             'n_estimators': [150, 200, 300],\n",
    "             'criterion' : ['gini', 'entropy']\n",
    "             }\n",
    "\n",
    "gs = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=3, scoring = 'accuracy')\n",
    "gs.fit(X_train, y_train)\n",
    "y_pred = gs.predict(X_test)\n",
    "print(\"Training Accuracy: {:.2f}\".format(gs.score(X_train, y_train)))\n",
    "print(\"Test Accuracy: {:.2f}\".format(gs.score(X_test, y_test)))\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6759199263743797\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.56      0.65      1510\n",
      "     neutral       0.56      0.74      0.63      1630\n",
      "    positive       0.78      0.72      0.75      1745\n",
      "\n",
      "    accuracy                           0.68      4885\n",
      "   macro avg       0.70      0.67      0.68      4885\n",
      "weighted avg       0.70      0.68      0.68      4885\n",
      "\n",
      "[[ 848  537  125]\n",
      " [ 191 1200  239]\n",
      " [  73  416 1256]]\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_test,y_pred, average='macro'))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDFVectorizer Models\n",
    "\n",
    "TFIDFVectorizer on the other hand is different from a CountVectorizer. The TFIDFVectorizer tries to avoid any frequently appearing words in relation to each doument. Words that don't appear as often in documents are seen as more valuble, while those that appear often are penalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfvec = TfidfVectorizer(ngram_range=(1,3),min_df=5)\n",
    "X = tfvec.fit_transform(df['clean_tweets'])\n",
    "X = X.tocsc()\n",
    "y = df['sentiment']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.20, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Multinomial Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.72\n",
      "Test Accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#initiate classifier\n",
    "nbc = MultinomialNB(alpha=10)\n",
    "nbc.fit(X_train, y_train)\n",
    "y_pred = nbc.predict(X_test)\n",
    "\n",
    "print(\"Training Accuracy: {:.2f}\".format(nbc.score(X_train, y_train)))\n",
    "print(\"Test Accuracy: {:.2f}\".format(nbc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6448989374888093\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.65      0.67      1510\n",
      "     neutral       0.59      0.49      0.54      1630\n",
      "    positive       0.67      0.81      0.73      1745\n",
      "\n",
      "    accuracy                           0.65      4885\n",
      "   macro avg       0.65      0.65      0.64      4885\n",
      "weighted avg       0.65      0.65      0.65      4885\n",
      "\n",
      "[[ 978  325  207]\n",
      " [ 333  797  500]\n",
      " [ 106  226 1413]]\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_test,y_pred, average='macro'))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.81\n",
      "Test Accuracy: 0.69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'gamma': 'scale', 'kernel': 'linear'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svc = SVC()\n",
    "param_grid = {'C': [ 1, 10, 20],\n",
    "             'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "              'gamma' : ['scale', 'auto']}\n",
    "\n",
    "\n",
    "gs = GridSearchCV(estimator=svc, param_grid=param_grid, cv=3, scoring = 'accuracy')\n",
    "gs.fit(X_train, y_train)\n",
    "y_pred = gs.predict(X_test)\n",
    "print(\"Training Accuracy: {:.2f}\".format(gs.score(X_train, y_train)))\n",
    "print(\"Test Accuracy: {:.2f}\".format(gs.score(X_test, y_test)))\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6932216271144324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.70      0.66      0.68      1510\n",
      "     neutral       0.60      0.67      0.63      1630\n",
      "    positive       0.79      0.74      0.76      1745\n",
      "\n",
      "    accuracy                           0.69      4885\n",
      "   macro avg       0.70      0.69      0.69      4885\n",
      "weighted avg       0.70      0.69      0.70      4885\n",
      "\n",
      "[[ 994  402  114]\n",
      " [ 299 1099  232]\n",
      " [ 118  333 1294]]\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_test,y_pred, average='macro'))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifer with Grid Seach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.76\n",
      "Test Accuracy: 0.67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 30,\n",
       " 'max_features': 'sqrt',\n",
       " 'n_estimators': 300}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "tfvec = TfidfVectorizer(ngram_range=(1,3),min_df=5)\n",
    "X = tfvec.fit_transform(df['clean_tweets'])\n",
    "X = X.tocsc()\n",
    "y = df['sentiment']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.20, random_state=1)\n",
    "\n",
    "#initiate classifier\n",
    "\n",
    "param_grid = {'max_depth': [10,20,30],\n",
    "             'max_features': ['sqrt', 'log2'],\n",
    "             'n_estimators': [150, 200, 300],\n",
    "             'criterion' : ['gini', 'entropy']\n",
    "             }\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "gs = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=3, scoring = 'accuracy')\n",
    "gs.fit(X_train, y_train)\n",
    "y_pred = gs.predict(X_test)\n",
    "print(\"Training Accuracy: {:.2f}\".format(gs.score(X_train, y_train)))\n",
    "print(\"Test Accuracy: {:.2f}\".format(gs.score(X_test, y_test)))\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6725121120809634\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.56      0.64      1510\n",
      "     neutral       0.55      0.73      0.63      1630\n",
      "    positive       0.77      0.72      0.74      1745\n",
      "\n",
      "    accuracy                           0.67      4885\n",
      "   macro avg       0.70      0.67      0.67      4885\n",
      "weighted avg       0.70      0.67      0.67      4885\n",
      "\n",
      "[[ 843  537  130]\n",
      " [ 195 1190  245]\n",
      " [  67  424 1254]]\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_test,y_pred, average='macro'))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection Summary \n",
    "\n",
    "## Best Model \n",
    "\n",
    "The best performing machine learning model was the Support Vector Classifier. The Support Vector Classifier achieved an accuracy of .70% and F1 score of .69%. The best hyperparameters found were â€˜Câ€™  set to 1, â€˜gammaâ€™ set to scale, and â€˜kernelâ€™ set to sigmoid. This model also had similar measures between precision and recall throughout all 3 classes. After looking at the confusion matrix for this model it appears that the class it struggled most with was the â€˜neutralâ€™. In my opinion this makes sense, because even from a human's perspective determining whether someone's emotions are neutral is difficult. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
